#!/usr/bin/env python3
"""
Reddit Scraper - æŠ•èµ„ç­–ç•¥å‘ç°
ä» Reddit æŠ•èµ„ç›¸å…³ subreddits è·å–è®¨è®ºå¹¶æå–ç­–ç•¥
"""

import os
from pathlib import Path
import sys
import json
import logging
import re
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass

import requests
from dotenv import load_dotenv

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.resolve()))

load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Path(__file__).parent / 'logs' / 'reddit.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('reddit_scraper')

@dataclass
class RedditPost:
    """Reddit å¸–å­"""
    id: str
    subreddit: str
    author: str
    title: str
    content: str
    url: str
    score: int
    num_comments: int
    created_at: str

class RedditScraper:
    """Reddit çˆ¬è™«"""
    
    def __init__(self):
        self.client_id = os.getenv('REDDIT_CLIENT_ID')
        self.client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        self.user_agent = os.getenv('REDDIT_USER_AGENT', 'StrategyMiner/1.0')
        self.subreddits = [
            'investing',
            'stocks',
            'wallstreetbets',
            'SecurityAnalysis',
            'options',
            'cryptomarkets',
            'CryptoMoonShots',
            'CryptoCurrency',
            'Bitcoin',
            'Trading',
            'Forex',
            'Daytrading'
        ]
        self.base_url = "https://www.reddit.com"
        self.auth = (self.client_id, self.client_secret) if self.client_id else None
        self.headers = {"User-Agent": self.user_agent}
    
    def fetch_posts(self, subreddit: str, sort: str = 'hot', limit: int = 50) -> List[Dict]:
        """è·å–å¸–å­"""
        if not self.auth:
            logger.warning(f"Reddit API æœªé…ç½®ï¼Œè·³è¿‡ r/{subreddit}")
            return []
        
        try:
            params = {"limit": limit, "sort": sort}
            response = requests.get(
                f"{self.base_url}/r/{subreddit}/{sort}.json",
                headers=self.headers,
                auth=self.auth,
                params=params,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {}).get('children', [])
            elif response.status_code == 401:
                logger.error(f"Reddit API è®¤è¯å¤±è´¥")
                return []
            else:
                logger.warning(f"Reddit API è¿”å›çŠ¶æ€ç : {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"è·å– r/{subreddit} å¤±è´¥: {e}")
            return []
    
    def extract_strategy_logic(self, title: str, content: str) -> Optional[str]:
        """ä»å¸–å­ä¸­æå–ç­–ç•¥é€»è¾‘"""
        full_text = f"{title} {content}"
        
        # ç­–ç•¥å…³é”®è¯æ¨¡å¼
        strategy_patterns = [
            # äº¤æ˜“æ¡ä»¶
            r'(?:buy|long|entry)\s+(?:when|if|on|at|above|over)\s+\S+',
            r'(?:sell|short|exit)\s+(?:when|if|on|at|below|under)\s+\S+',
            r'(?:go|long|open)\s+(?:long|short|position)\s+(?:when|if)\s+\S+',
            
            # æŠ€æœ¯æŒ‡æ ‡
            r'(?:ma|moving average|sma|ema)\s*\d*',
            r'\d+[- ]?day\s*(?:ma|moving average|sma|ema)',
            r'(?:golden cross|death cross)',
            r'(?:rsi)\s*(?:below|under|<|above|over|>)\s*\d+',
            r'(?:macd)\s*(?:cross|signal|histogram)',
            r'(?:bollinger|bb)\s*(?:bands?|upper|lower|middle)',
            r'(?:volume)\s*(?:spike|surge|expansion)',
            r'(?:support|resistance|support level|resistance level)',
            r'(?:breakout|breakdown|pullback|reversal)',
            
            # äº¤æ˜“è§„åˆ™
            r'(?:stop[- ]?loss|sl)\s*(?:at|to|-)\s*\d+%',
            r'(?:take[- ]?profit|tp)\s*(?:at|to|-)\s*\d+%',
            r'(?:risk:?|reward:?)\s*\d+[:to]+\d+',
            r'(?:position\s*size|size)\s*(?:\d+%|\d+[- ]?percent)',
            
            # ç­–ç•¥ç±»å‹
            r'(?:momentum|trend[- ]?following|mean[- ]?reversion)',
            r'(?:scalping|swing\s*trading|day\s*trading)',
            r'(?:value\s*investing|growth\s*investing)',
            r'(?:options?\s*(?:strategy|play|call|put))',
            r'(?:straddle|strangle|iron\s*condor|butterfly)',
            
            # é‡åŒ–è§„åˆ™
            r'(?:backtest|back[- ]?test)\s*(?:showed|revealed|indicated)',
            r'(?:win\s*rate|winning\s*percentage)\s*(?:\d+%|â‰¥|>=)',
            r'(?:profit\s*factor|expectancy)',
            r'(?:indicator|signal|trigger)',
            
            # å…·ä½“æ•°å€¼
            r'\d+%\s*(?:gain|profit|return|move|up|down|rally|dip)',
            r'(?:until|until\s*then|then)',
        ]
        
        for pattern in strategy_patterns:
            match = re.search(pattern, full_text, re.IGNORECASE)
            if match:
                # è·å–ä¸Šä¸‹æ–‡
                start = max(0, match.start() - 50)
                end = min(len(full_text), match.end() + 80)
                context = full_text[start:end].strip()
                
                # æ¸…ç†æ ¼å¼
                context = re.sub(r'\s+', ' ', context)
                
                # å¦‚æœä¸Šä¸‹æ–‡å¤ªçŸ­æˆ–å¤ªé•¿ï¼Œå°è¯•æ‰©å±•
                if len(context) < 20:
                    continue
                
                logger.info(f"  æå–åˆ°ç­–ç•¥é€»è¾‘: {context[:100]}...")
                return context
        
        return None
    
    def contains_strategy_keywords(self, title: str, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç­–ç•¥å…³é”®è¯"""
        keywords = [
            'strategy', 'method', 'approach', 'technique',
            'setup', 'pattern', 'trade', 'trading',
            'indicator', 'signal', 'system',
            'buy when', 'sell when', 'long when', 'short when',
            'entry', 'exit', 'position',
            'backtest', 'results', 'performance',
            'winning', 'profit', 'roi',
            'moving average', 'rsi', 'macd', 'bollinger',
            'stop loss', 'take profit', 'risk reward',
        ]
        
        text = f"{title} {content}".lower()
        matches = sum(1 for kw in keywords if kw in text)
        return matches >= 2  # è‡³å°‘åŒ¹é…2ä¸ªå…³é”®è¯
    
    def analyze_post(self, post_data: Dict) -> Optional[Dict]:
        """åˆ†æå•ä¸ªå¸–å­"""
        post = post_data.get('data', {})
        
        title = post.get('title', '')
        content = post.get('selftext', '')
        score = post.get('score', 0)
        num_comments = post.get('num_comments', 0)
        
        # è¿‡æ»¤ä½è´¨é‡å¸–å­
        if score < 10:
            return None
        
        if not self.contains_strategy_keywords(title, content):
            return None
        
        # æå–ç­–ç•¥é€»è¾‘
        logic = self.extract_strategy_logic(title, content)
        
        if not logic:
            return None
        
        return {
            'id': post.get('id'),
            'subreddit': post.get('subreddit', '').replace('r/', ''),
            'author': post.get('author', 'unknown'),
            'title': title[:200],
            'content': f"{title} {content}",
            'url': f"https://reddit.com{post.get('permalink', '')}",
            'score': score,
            'num_comments': num_comments,
            'created_at': datetime.fromtimestamp(post.get('created_utc', 0)).isoformat(),
            'extracted_logic': logic
        }
    
    def scan_subreddit(self, subreddit: str) -> List[Dict]:
        """æ‰«æå•ä¸ª subreddit"""
        logger.info(f"ğŸ“º æ‰«æ r/{subreddit}...")
        
        all_posts = []
        
        # è·å–çƒ­é—¨å¸–å­
        posts = self.fetch_posts(subreddit, sort='hot', limit=50)
        all_posts.extend(posts)
        
        # è·å–æ–°å¸–å­
        posts = self.fetch_posts(subreddit, sort='new', limit=50)
        all_posts.extend(posts)
        
        # å»é‡
        seen_ids = set()
        unique_posts = []
        for post in all_posts:
            post_id = post.get('data', {}).get('id')
            if post_id and post_id not in seen_ids:
                seen_ids.add(post_id)
                unique_posts.append(post)
        
        logger.info(f"  æ‰¾åˆ° {len(unique_posts)} ä¸ªå”¯ä¸€å¸–å­")
        
        # åˆ†ææ¯ä¸ªå¸–å­
        strategies = []
        for post in unique_posts:
            analysis = self.analyze_post(post)
            if analysis:
                strategies.append(analysis)
                logger.info(f"  âœ… å‘ç°ç­–ç•¥: {analysis['title'][:60]}...")
        
        logger.info(f"  ä» r/{subreddit} å‘ç° {len(strategies)} ä¸ªç­–ç•¥")
        return strategies
    
    def scan_all(self) -> List[Dict]:
        """æ‰«ææ‰€æœ‰ subreddit"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹ Reddit ç­–ç•¥æ‰«æ...")
        logger.info(f"ç›®æ ‡ subreddits: {', '.join(self.subreddits)}")
        logger.info("=" * 60)
        
        all_strategies = []
        
        for subreddit in self.subreddits:
            try:
                strategies = self.scan_subreddit(subreddit)
                all_strategies.extend(strategies)
            except Exception as e:
                logger.error(f"æ‰«æ r/{subreddit} å¤±è´¥: {e}")
        
        # å»é‡ï¼ˆåŸºäº extracted_logicï¼‰
        seen_logics = set()
        unique_strategies = []
        for s in all_strategies:
            logic_key = s['extracted_logic'][:100].lower()
            if logic_key not in seen_logics:
                seen_logics.add(logic_key)
                unique_strategies.append(s)
        
        logger.info("\n" + "=" * 60)
        logger.info(f"ğŸ“Š Reddit æ‰«æå®Œæˆ")
        logger.info(f"   æ€»å‘ç°: {len(all_strategies)}")
        logger.info(f"   å»é‡å: {len(unique_strategies)}")
        logger.info("=" * 60)
        
        return unique_strategies
    
    def save_strategies(self, strategies: List[Dict]):
        """ä¿å­˜ç­–ç•¥åˆ° strategies.json"""
        strategies_file = Path(__file__).parent / 'strategies.json'
        
        try:
            with open(strategies_file, 'r') as f:
                data = json.load(f)
        except FileNotFoundError:
            data = {"strategies": [], "metadata": {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "total_scanned": 0,
                "passed": 0,
                "rejected": 0
            }}
        
        existing_urls = {s.get('url') for s in data['strategies']}
        added_count = 0
        
        for strategy in strategies:
            if strategy['url'] in existing_urls:
                continue
            
            new_strategy = {
                'id': len(data['strategies']) + 1,
                'source': 'reddit',
                'author': strategy['author'],
                'url': strategy['url'],
                'title': strategy['title'],
                'content': strategy['content'],
                'extracted_logic': strategy['extracted_logic'],
                'discovered_at': strategy['created_at'],
                'validated_at': None,
                'status': 'pending',
                'backtest_result': None,
                'keywords': [strategy['subreddit']],
                'data_source': 'reddit',
                'score': strategy['score'],
                'num_comments': strategy['num_comments']
            }
            
            data['strategies'].append(new_strategy)
            data['metadata']['total_scanned'] += 1
            data['metadata']['last_updated'] = datetime.now().isoformat()
            added_count += 1
        
        with open(strategies_file, 'w') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ä¿å­˜äº† {added_count} ä¸ªæ–°ç­–ç•¥åˆ° {strategies_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Reddit Strategy Scraper')
    parser.add_argument('--subreddit', '-s', type=str, help='Specific subreddit to scan')
    parser.add_argument('--output', '-o', type=str, default='strategies.json', 
                       help='Output file path')
    args = parser.parse_args()
    
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs(Path(__file__).parent / 'logs', exist_ok=True)
    
    scraper = RedditScraper()
    
    if args.subreddit:
        # æ‰«æå•ä¸ª subreddit
        strategies = scraper.scan_subreddit(args.subreddit)
    else:
        # æ‰«ææ‰€æœ‰
        strategies = scraper.scan_all()
    
    # ä¿å­˜
    scraper.save_strategies(strategies)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nğŸ“ˆ å‘ç° {len(strategies)} ä¸ªç­–ç•¥:")
    for i, s in enumerate(strategies[:10], 1):
        print(f"{i}. [{s['subreddit']}] {s['title'][:60]}...")
        print(f"   é€»è¾‘: {s['extracted_logic'][:80]}...")
    
    if len(strategies) > 10:
        print(f"... è¿˜æœ‰ {len(strategies) - 10} ä¸ªç­–ç•¥")


if __name__ == "__main__":
    main()
