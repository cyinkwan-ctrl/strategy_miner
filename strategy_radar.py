#!/usr/bin/env python3
"""
ç­–ç•¥é›·è¾¾ç›‘æŽ§æ¨¡å—
æ•´åˆ RSS å’Œ Playwright å®žçŽ°é›¶æˆæœ¬çš„ X/Twitter æ•°æ®é‡‡é›†
"""

import os
from pathlib import Path
import sys
import json
import logging
import re
from datetime import datetime
from typing import List, Dict, Optional, Union
from dataclasses import dataclass, asdict
import requests
from dotenv import load_dotenv

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.resolve()))

# å¯¼å…¥æ–°æ¨¡å—
try:
    from x_rss_scanner import XRSSScanner, TweetItem as RSSTweet
    from x_playwright_scraper import XPlaywrightScraper, TweetItem as PWTweet
    RSS_AVAILABLE = True
except ImportError as e:
    RSS_AVAILABLE = False
    logger.warning(f"æ–°æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# åŠ è½½é…ç½®
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Path(__file__).parent / 'logs' / 'radar.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('strategy_radar')

@dataclass
class StrategyCandidate:
    """ç­–ç•¥å€™é€‰"""
    source: str  # æ¥æºï¼štwitter_rss/twitter_playwright/reddit
    author: str  # ä½œè€…
    url: str  # åŽŸæ–‡é“¾æŽ¥
    title: str  # æ ‡é¢˜
    content: str  # åŽŸå§‹å†…å®¹
    extracted_logic: str  # æå–çš„ç­–ç•¥é€»è¾‘
    discovered_at: str
    keywords: List[str]
    data_source: str = "rss"  # æ•°æ®æºç±»åž‹

class StrategyRadar:
    """ç­–ç•¥é›·è¾¾ä¸»ç±»"""
    
    def __init__(self):
        self.config_file = Path(__file__).parent / 'monitored_accounts.json'
        self.strategies_file = Path(__file__).parent / 'strategies.json'
        self.rss_scanner = None
        self.playwright_scraper = None
        
        # åˆå§‹åŒ–æ‰«æå™¨
        if RSS_AVAILABLE:
            try:
                self.rss_scanner = XRSSScanner(self.config_file)
                logger.info("âœ… RSS æ‰«æå™¨å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"âŒ RSS æ‰«æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            
            try:
                self.playwright_scraper = XPlaywrightScraper(self.config_file)
                logger.info("âœ… Playwright æŠ“å–å™¨å·²åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"âŒ Playwright æŠ“å–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("âš ï¸ æ–°æ‰«ææ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼")
        
        # ä¼ ç»Ÿ Reddit æ‰«æå™¨
        self.reddit_scanner = RedditScanner()
    
    def _extract_strategy_logic(self, content: str) -> Optional[str]:
        """ä»Žå†…å®¹ä¸­æå–ç­–ç•¥é€»è¾‘"""
        # ç­–ç•¥å…³é”®è¯æ¨¡å¼
        strategy_patterns = [
            r'(?:buy|long|sell|short)\s+(?:when|if|on|at)\s+\S+',
            r'(?:moving average|ma|ema|sma)\s*\d*',
            r'(?:rsi|macd|bollinger|support|resistance)',
            r'(?:stop[- ]?loss|take[- ]?profit|tp|sl)',
            r'(?:breakout|pullback|reversal)',
            r'(?:entry|exit|target|setup)',
            r'(?:long\s+(?:position|entry)|short\s+(?:position|entry))',
            r'\d+%\s*(?:gain|profit|return|move)',
        ]
        
        for pattern in strategy_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                # èŽ·å–ä¸Šä¸‹æ–‡
                start = max(0, match.start() - 30)
                end = min(len(content), match.end() + 50)
                return content[start:end].strip()
        
        return content[:100] if content else None
    
    def _convert_rss_tweet(self, tweet: RSSTweet) -> StrategyCandidate:
        """è½¬æ¢ RSS æŽ¨æ–‡ä¸ºç­–ç•¥å€™é€‰"""
        return StrategyCandidate(
            source="twitter_rss",
            author=tweet.author,
            url=tweet.url,
            title=tweet.title,
            content=tweet.content,
            extracted_logic=self._extract_strategy_logic(tweet.content),
            discovered_at=datetime.now().isoformat(),
            keywords=[],
            data_source="rss"
        )
    
    def _convert_pw_tweet(self, tweet: PWTweet) -> StrategyCandidate:
        """è½¬æ¢ Playwright æŽ¨æ–‡ä¸ºç­–ç•¥å€™é€‰"""
        return StrategyCandidate(
            source="twitter_playwright",
            author=tweet.author,
            url=tweet.url,
            title=tweet.title,
            content=tweet.content,
            extracted_logic=self._extract_strategy_logic(tweet.content),
            discovered_at=datetime.now().isoformat(),
            keywords=[],
            data_source="playwright"
        )
    
    def scan_via_rss(self) -> List[StrategyCandidate]:
        """é€šè¿‡ RSS æ‰«æ"""
        if not self.rss_scanner:
            logger.warning("RSS æ‰«æå™¨ä¸å¯ç”¨")
            return []
        
        try:
            results = self.rss_scanner.scan_all()
            candidates = []
            
            for username, tweets in results.items():
                for tweet in tweets:
                    candidate = self._convert_rss_tweet(tweet)
                    if candidate.extracted_logic:
                        candidates.append(candidate)
                        logger.info(f"ðŸ“° RSS å‘çŽ°ç­–ç•¥: @{username} - {candidate.extracted_logic[:50]}...")
            
            logger.info(f"âœ… RSS æ‰«æå®Œæˆï¼Œå‘çŽ° {len(candidates)} ä¸ªç­–ç•¥")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ RSS æ‰«æå¤±è´¥: {e}")
            return []
    
    def scan_via_playwright(self) -> List[StrategyCandidate]:
        """é€šè¿‡ Playwright æ‰«æ"""
        if not self.playwright_scraper:
            logger.warning("Playwright æŠ“å–å™¨ä¸å¯ç”¨")
            return []
        
        try:
            results = self.playwright_scraper.scan_all()
            candidates = []
            
            for username, tweets in results.items():
                for tweet in tweets:
                    candidate = self._convert_pw_tweet(tweet)
                    if candidate.extracted_logic:
                        candidates.append(candidate)
                        logger.info(f"ðŸ“° Playwright å‘çŽ°ç­–ç•¥: @{username} - {candidate.extracted_logic[:50]}...")
            
            logger.info(f"âœ… Playwright æ‰«æå®Œæˆï¼Œå‘çŽ° {len(candidates)} ä¸ªç­–ç•¥")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ Playwright æ‰«æå¤±è´¥: {e}")
            return []
    
    def load_existing_strategies(self) -> set:
        """åŠ è½½å·²å­˜åœ¨çš„ç­–ç•¥URL"""
        try:
            with open(self.strategies_file, 'r') as f:
                data = json.load(f)
                return {s.get('url') for s in data.get('strategies', [])}
        except FileNotFoundError:
            return set()
    
    def save_strategy_candidate(self, candidate: StrategyCandidate):
        """ä¿å­˜ç­–ç•¥å€™é€‰åˆ°å¾…éªŒè¯åˆ—è¡¨"""
        try:
            # è¯»å–çŽ°æœ‰æ•°æ®
            with open(self.strategies_file, 'r') as f:
                data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            for strategy in data['strategies']:
                if strategy.get('url') == candidate.url:
                    logger.info(f"ç­–ç•¥å·²å­˜åœ¨ï¼Œè·³è¿‡: {candidate.url}")
                    return
            
            # æ·»åŠ æ–°ç­–ç•¥
            new_strategy = {
                'id': len(data['strategies']) + 1,
                'source': candidate.source,
                'author': candidate.author,
                'url': candidate.url,
                'title': candidate.title,
                'content': candidate.content,
                'extracted_logic': candidate.extracted_logic,
                'discovered_at': candidate.discovered_at,
                'validated_at': None,
                'status': 'pending',  # pending, passed, rejected
                'backtest_result': None,
                'keywords': candidate.keywords,
                'data_source': candidate.data_source
            }
            
            data['strategies'].append(new_strategy)
            data['metadata']['total_scanned'] += 1
            data['metadata']['last_updated'] = datetime.now().isoformat()
            data['metadata']['sources_used'].append(candidate.source) if candidate.source not in data['metadata']['sources_used'] else None
            
            # ä¿å­˜
            with open(self.strategies_file, 'w') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ðŸ’¾ ç­–ç•¥å·²ä¿å­˜: {candidate.title}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç­–ç•¥å¤±è´¥: {e}")
    
    def scan_all(self) -> List[StrategyCandidate]:
        """æ‰§è¡Œå…¨é‡æ‰«æï¼ˆRSS + Playwrightï¼‰"""
        logger.info("=" * 60)
        logger.info("ðŸš€ å¼€å§‹ç­–ç•¥é›·è¾¾æ‰«æï¼ˆé›¶æˆæœ¬æ–¹æ¡ˆï¼‰...")
        logger.info("=" * 60)
        
        all_candidates = []
        existing_urls = self.load_existing_strategies()
        
        # 1. RSS æ‰«æï¼ˆä¼˜å…ˆï¼Œæ›´è½»é‡ï¼‰
        logger.info("\nðŸ“¡ é˜¶æ®µ 1: RSS æ‰«æ...")
        rss_candidates = self.scan_via_rss()
        for candidate in rss_candidates:
            if candidate.url not in existing_urls:
                all_candidates.append(candidate)
        
        # 2. Playwright æ‰«æï¼ˆé’ˆå¯¹æ—  RSS çš„è´¦å·ï¼‰
        logger.info("\nðŸŒ é˜¶æ®µ 2: Playwright æ‰«æ...")
        pw_candidates = self.scan_via_playwright()
        for candidate in pw_candidates:
            if candidate.url not in existing_urls:
                all_candidates.append(candidate)
        
        # 3. Reddit æ‰«æï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
        logger.info("\nðŸ“º é˜¶æ®µ 3: Reddit æ‰«æ...")
        reddit_candidates = self.reddit_scanner.scan()
        for candidate in reddit_candidates:
            if candidate.url not in existing_urls:
                all_candidates.append(candidate)
        
        # ä¿å­˜æ–°å‘çŽ°çš„ç­–ç•¥
        for candidate in all_candidates:
            self.save_strategy_candidate(candidate)
        
        logger.info("\n" + "=" * 60)
        logger.info(f"ðŸ“Š æ‰«æå®Œæˆ")
        logger.info(f"   RSS ç­–ç•¥: {len(rss_candidates)}")
        logger.info(f"   Playwright ç­–ç•¥: {len(pw_candidates)}")
        logger.info(f"   Reddit ç­–ç•¥: {len(reddit_candidates)}")
        logger.info(f"   æ–°ç­–ç•¥æ€»æ•°: {len(all_candidates)}")
        logger.info("=" * 60)
        
        return all_candidates
    
    def get_source_stats(self) -> Dict:
        """èŽ·å–æ•°æ®æºç»Ÿè®¡"""
        stats = {
            "rss_accounts": 0,
            "playwright_accounts": 0,
            "total_accounts": 0
        }
        
        try:
            with open(self.config_file, 'r') as f:
                data = json.load(f)
                accounts = data.get('accounts', [])
                
                stats["total_accounts"] = len(accounts)
                stats["rss_accounts"] = len([a for a in accounts if a.get('source') == 'rss'])
                stats["playwright_accounts"] = len([a for a in accounts if a.get('source') == 'playwright'])
                
        except Exception as e:
            logger.error(f"èŽ·å–ç»Ÿè®¡å¤±è´¥: {e}")
        
        return stats


class RedditScanner:
    """Reddit æ‰«æå™¨ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼Œä¿ç•™å¤‡ç”¨ï¼‰"""
    
    def __init__(self):
        self.client_id = os.getenv('REDDIT_CLIENT_ID')
        self.client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        self.user_agent = os.getenv('REDDIT_USER_AGENT', 'StrategyMiner')
        self.subreddits = ['CryptoMoonShots', 'CryptoCurrency', 'Bitcoin', 'ethfinance']
        self.base_url = "https://www.reddit.com"
    
    def fetch_hot_posts(self, subreddit: str) -> List[Dict]:
        """èŽ·å–çƒ­é—¨å¸–å­"""
        if not self.client_id:
            logger.warning("Reddit API æœªé…ç½®ï¼Œè·³è¿‡")
            return []
        
        try:
            headers = {"User-Agent": self.user_agent}
            auth = (self.client_id, self.client_secret)
            params = {"limit": 20, "sort": "hot"}
            
            response = requests.get(
                f"{self.base_url}/r/{subreddit}/new.json",
                headers=headers, auth=auth, params=params, timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get('data', {}).get('children', [])
            return []
            
        except Exception as e:
            logger.error(f"Reddit èŽ·å–å¤±è´¥: {e}")
            return []
    
    def extract_strategy_logic(self, post: Dict) -> Optional[str]:
        """ä»Žå¸–å­ä¸­æå–ç­–ç•¥é€»è¾‘"""
        title = post.get('title', '')
        self_text = post.get('selftext', '')
        full_text = f"{title} {self_text}"
        
        strategy_patterns = [
            r'(?:buy|long|sell|short)\s+(?:when|if|on|at)\s+\S+',
            r'(?:moving average|ma|ema|sma|rsi|macd|bollinger)',
            r'(?:stop[- ]?loss|take[- ]?profit|tp|sl)',
            r'(?:strategy|method|approach|technique)',
            r'\d+%\s*(?:gain|profit|return|stop)',
        ]
        
        for pattern in strategy_patterns:
            match = re.search(pattern, full_text, re.IGNORECASE)
            if match:
                start = max(0, match.start() - 30)
                end = min(len(full_text), match.end() + 50)
                return full_text[start:end].strip()
        
        return None
    
    def scan(self) -> List[StrategyCandidate]:
        """æ‰§è¡Œæ‰«æ"""
        candidates = []
        
        for subreddit in self.subreddits:
            logger.info(f"æ‰«æ Reddit: r/{subreddit}")
            posts = self.fetch_hot_posts(subreddit)
            
            for post in posts:
                post_data = post.get('data', {})
                logic = self.extract_strategy_logic(post_data)
                
                if logic:
                    candidate = StrategyCandidate(
                        source="reddit",
                        author=post_data.get('author', 'unknown'),
                        url=f"https://reddit.com{post_data.get('permalink', '')}",
                        title=post_data.get('title', '')[:100],
                        content=f"{post_data.get('title', '')} {post_data.get('selftext', '')}",
                        extracted_logic=logic,
                        discovered_at=datetime.now().isoformat(),
                        keywords=[subreddit],
                        data_source="reddit"
                    )
                    candidates.append(candidate)
        
        logger.info(f"Reddit æ‰«æå®Œæˆï¼Œå‘çŽ° {len(candidates)} ä¸ªç­–ç•¥")
        return candidates


if __name__ == "__main__":
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs(Path(__file__).parent / 'logs', exist_ok=True)
    
    radar = StrategyRadar()
    
    # æ˜¾ç¤ºæ•°æ®æºç»Ÿè®¡
    stats = radar.get_source_stats()
    print("\n" + "="*60)
    print("ðŸ”” ç­–ç•¥é›·è¾¾ï¼ˆé›¶æˆæœ¬æ•°æ®é‡‡é›†æ–¹æ¡ˆï¼‰")
    print("="*60)
    print(f"ðŸ“Š æ•°æ®æºç»Ÿè®¡:")
    print(f"   RSS è´¦å·: {stats['rss_accounts']}")
    print(f"   Playwright è´¦å·: {stats['playwright_accounts']}")
    print(f"   æ€»è®¡: {stats['total_accounts']}")
    
    # æ‰§è¡Œæ‰«æ
    candidates = radar.scan_all()
    
    print(f"\nðŸ“ˆ å‘çŽ° {len(candidates)} ä¸ªæ–°ç­–ç•¥å€™é€‰:")
    for i, c in enumerate(candidates[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"{i}. [{c.source}] @{c.author}")
        print(f"   é€»è¾‘: {c.extracted_logic[:80]}...")
    
    if len(candidates) > 5:
        print(f"... è¿˜æœ‰ {len(candidates) - 5} ä¸ªç­–ç•¥")
