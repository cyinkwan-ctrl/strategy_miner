#!/usr/bin/env python3
"""
X/Twitter Playwright æŠ“å–å™¨
æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨æŠ“å–æ¨æ–‡ï¼ˆé’ˆå¯¹æ—  RSS çš„è´¦å·ï¼‰
é›¶æˆæœ¬ã€ä½¿ç”¨ nitter.net ä½œä¸ºå¤‡é€‰
"""

import os
from pathlib import Path
import sys
import json
import logging
import re
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from playwright.sync_api import sync_playwright

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.resolve()))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('x_playwright_scraper')

@dataclass
class TweetItem:
    """æ¨æ–‡æ•°æ®"""
    author: str
    url: str
    title: str
    content: str
    published_at: str
    source: str = "playwright"

# Nitter å®ä¾‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
NITTER_INSTANCES = [
    "nitter.net",
    "nitter.privacydev.net",
    "nitter.poast.org",
    "nitter.moomoo.me",
    "nitter.tedomum.net",
]

class XPlaywrightScraper:
    """X/Twitter Playwright æŠ“å–å™¨"""
    
    def __init__(self, config_file: str = None):
        self.config_file = config_file or Path(__file__).parent / 'monitored_accounts.json'
        self.accounts = self._load_accounts()
        self.browser = None
        self.context = None
    
    def _load_accounts(self) -> List[Dict]:
        """åŠ è½½ç›‘æ§è´¦å·é…ç½®"""
        try:
            with open(self.config_file, 'r') as f:
                data = json.load(f)
                return data.get('accounts', [])
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}")
            return []
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return []
    
    def _get_nitter_url(self, username: str) -> str:
        """è·å– Nitter URL"""
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ Nitter å®ä¾‹
        instance = NITTER_INSTANCES[0]
        return f"https://{instance}/{username}"
    
    def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if self.browser:
            return
        
        logger.info("ğŸš€ å¯åŠ¨ Playwright æµè§ˆå™¨...")
        
        playwright = sync_playwright().start()
        
        # å¯åŠ¨æ— å¤´æµè§ˆå™¨
        self.browser = playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--window-size=1920,1080',
                '--start-maximized',
            ]
        )
        
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
        )
        
        self.playwright = playwright
        logger.info("âœ… Playwright æµè§ˆå™¨å·²å¯åŠ¨")
    
    def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.close()
            self.playwright.stop()
            self.browser = None
            self.context = None
            logger.info("ğŸ”’ Playwright æµè§ˆå™¨å·²å…³é—­")
    
    def _check_page_loaded(self, page) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦åŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            page.wait_for_load_state('networkidle', timeout=10000)
            return True
        except Exception as e:
            logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
            return False
    
    def _extract_tweets_from_nitter(self, page) -> List[Dict]:
        """ä» Nitter é¡µé¢æå–æ¨æ–‡"""
        tweets = []
        
        try:
            # ä½¿ç”¨å¤šç§é€‰æ‹©å™¨å°è¯•æå–æ¨æ–‡
            selectors = [
                '.timeline-item',  # Nitter ç»å…¸é€‰æ‹©å™¨
                '.tweet',          # é€šç”¨é€‰æ‹©å™¨
                '[class*="tweet"]',  # åŒ…å« tweet çš„å…ƒç´ 
                'article',         # HTML5 article
            ]
            
            tweet_elements = []
            for selector in selectors:
                elements = page.query_selector_all(selector)
                if elements:
                    tweet_elements = elements
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
                    break
            
            for element in tweet_elements[:10]:  # åªå–æœ€æ–°10æ¡
                try:
                    # æå–æ¨æ–‡å†…å®¹
                    content_elem = element.query_selector('.tweet-content, .tweet-text, [class*="content"]')
                    content = content_elem.inner_text() if content_elem else element.inner_text()
                    
                    # æå–æ—¶é—´
                    time_elem = element.query_selector('.tweet-date, [class*="date"], time')
                    time_str = time_elem.get_attribute('title') or time_elem.inner_text() if time_elem else datetime.now().isoformat()
                    
                    # æå–é“¾æ¥
                    link_elem = element.query_selector('a.tweet-link, [href*="/status/"]')
                    link = link_elem.get_attribute('href') if link_elem else ''
                    if link and not link.startswith('http'):
                        link = f"https://nitter.net{link}"
                    
                    # æ¸…ç†å†…å®¹
                    content = content.strip()[:500] if content else ''
                    
                    if content and len(content) > 10:
                        tweets.append({
                            'content': content,
                            'time': time_str,
                            'link': link or 'https://nitter.net/unknown',
                        })
                        
                except Exception as e:
                    logger.debug(f"æå–å•ä¸ªæ¨æ–‡å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"æå–æ¨æ–‡å¤±è´¥: {e}")
        
        return tweets
    
    def _is_spam_or_promotion(self, content: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåƒåœ¾å¹¿å‘Šæˆ–æ¨å¹¿å†…å®¹"""
        spam_patterns = [
            r'(?:DM|dm|ç§ä¿¡).*?(?:è·å–|get|é¢†å–)',
            r'(?:å…è´¹|free).*?(?:èµ é€|é¢†å–|åŠ å¾®ä¿¡)',
            r'(?:æƒç¢¼|æ‰«ç |ç‚¹å‡»é“¾æ¥)',
            r'(?:ä»£å¸|token).*?(?:å‘è¡Œ|launch|å‘å°„)',
            r'(?:ç©ºæŠ•|airdrop).*?(?:é¢†å–|claim)',
            r'https?://t\.co/\S+',  # çŸ­é“¾æ¥
            r'(?:åŠ å¾®ä¿¡|wechat|å¾®ä¿¡)',
            r'(?:é‚€è¯·ç |referral).*?(?:å…è´¹|free)',
        ]
        
        for pattern in spam_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        
        return False
    
    def _is_retweet(self, content: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºè½¬å‘å†…å®¹"""
        rt_patterns = [
            r'^RT @',
            r'^è½¬å‘è‡ª',
            r'^âš ï¸.*?è½¬å‘',
            r'^MT @',
        ]
        
        for pattern in rt_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        
        return False
    
    def _extract_strategy_content(self, content: str, keywords: List[str]) -> Optional[str]:
        """ä»æ¨æ–‡å†…å®¹ä¸­æå–ç­–ç•¥ç›¸å…³ä¿¡æ¯"""
        if not keywords:
            return content[:200] if content else None
        
        content_lower = content.lower()
        keywords_lower = [k.lower() for k in keywords]
        
        for keyword in keywords_lower:
            if keyword in content_lower:
                idx = content_lower.find(keyword)
                start = max(0, idx - 50)
                end = min(len(content), idx + 100)
                return content[start:end].strip()
        
        return None
    
    def fetch_tweets_via_nitter(self, username: str, strategy_keywords: List[str] = None) -> List[TweetItem]:
        """é€šè¿‡ Nitter è·å–æ¨æ–‡"""
        tweets = []
        nitter_url = self._get_nitter_url(username)
        
        logger.info(f"ğŸŒ è®¿é—® Nitter: {nitter_url}")
        
        try:
            self._init_browser()
            page = self.context.new_page()
            
            # æ¨¡æ‹ŸçœŸå®è®¿é—®
            page.goto(nitter_url, wait_until='networkidle')
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            if not self._check_page_loaded(page):
                logger.warning(f"é¡µé¢åŠ è½½ä¸å®Œæ•´: {username}")
                return []
            
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
            import time
            time.sleep(2)
            
            # æå–æ¨æ–‡
            raw_tweets = self._extract_tweets_from_nitter(page)
            
            for raw in raw_tweets:
                content = raw.get('content', '')
                
                # è·³è¿‡è½¬å‘
                if self._is_retweet(content):
                    continue
                
                # è·³è¿‡åƒåœ¾å¹¿å‘Š
                if self._is_spam_or_promotion(content):
                    continue
                
                # æå–ç­–ç•¥å†…å®¹
                if strategy_keywords:
                    strategy_content = self._extract_strategy_content(content, strategy_keywords)
                    if not strategy_content:
                        continue
                else:
                    strategy_content = content[:200]
                
                tweet = TweetItem(
                    author=username,
                    url=raw.get('link', f'https://nitter.net/{username}'),
                    title=content[:100],
                    content=content,
                    published_at=raw.get('time', datetime.now().isoformat()),
                    source="playwright"
                )
                
                tweets.append(tweet)
                logger.info(f"ğŸ“° æå–æ¨æ–‡: {content[:50]}...")
            
            page.close()
            
        except Exception as e:
            logger.error(f"âŒ Nitter æŠ“å–å¤±è´¥: {username} - {e}")
        
        return tweets
    
    def fetch_tweets_via_web(self, username: str, strategy_keywords: List[str] = None) -> List[TweetItem]:
        """å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥é€šè¿‡ requests è·å–ç½‘é¡µ"""
        logger.info(f"ğŸŒ ä½¿ç”¨ requests å¤‡ç”¨æ–¹æ¡ˆ: @{username}")
        
        try:
            import requests
            from bs4 import BeautifulSoup
            
            nitter_url = self._get_nitter_url(username)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(nitter_url, headers=headers, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            tweet_elements = soup.select('.timeline-item, .tweet')[:10]
            
            tweets = []
            for elem in tweet_elements:
                try:
                    content_elem = elem.select_one('.tweet-content, .tweet-text')
                    content = content_elem.get_text(strip=True) if content_elem else ''
                    
                    if content and not self._is_retweet(content) and not self._is_spam_or_promotion(content):
                        if strategy_keywords:
                            strategy_content = self._extract_strategy_content(content, strategy_keywords)
                            if not strategy_content:
                                continue
                        else:
                            strategy_content = content[:200]
                        
                        tweets.append(TweetItem(
                            author=username,
                            url=f"https://nitter.net/{username}",
                            title=content[:100],
                            content=content,
                            published_at=datetime.now().isoformat(),
                            source="playwright"
                        ))
                except Exception:
                    continue
            
            return tweets
            
        except ImportError:
            logger.warning("BeautifulSoup ä¸å¯ç”¨ï¼Œè·³è¿‡å¤‡ç”¨æ–¹æ¡ˆ")
            return []
        except Exception as e:
            logger.error(f"å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
            return []
    
    def scan_account(self, account: Dict) -> List[TweetItem]:
        """æ‰«æå•ä¸ªè´¦å·"""
        username = account.get('username')
        strategy_keywords = account.get('strategy_keywords', [])
        
        if not username:
            return []
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ” Playwright æ‰«æè´¦å·: @{username}")
        logger.info(f"ğŸ“Š ç­–ç•¥å…³é”®è¯: {strategy_keywords}")
        
        # ä¼˜å…ˆä½¿ç”¨ Nitterï¼ˆé€šè¿‡ Playwrightï¼‰
        tweets = self.fetch_tweets_via_nitter(username, strategy_keywords)
        
        if not tweets:
            # å¤‡ç”¨ï¼šä½¿ç”¨ requests
            logger.info("ğŸ”„ å°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
            tweets = self.fetch_tweets_via_web(username, strategy_keywords)
        
        logger.info(f"âœ… è·å– {len(tweets)} æ¡ç­–ç•¥ç›¸å…³æ¨æ–‡")
        return tweets
    
    def scan_all(self) -> Dict[str, List[TweetItem]]:
        """æ‰«ææ‰€æœ‰é…ç½®è´¦å·ï¼ˆåªæ‰«æé…ç½®ä¸º Playwright çš„è´¦å·ï¼‰"""
        results = {}
        
        try:
            for account in self.accounts:
                source = account.get('source', 'playwright')
                
                if source != 'playwright':
                    logger.info(f"â­ï¸ è·³è¿‡é Playwright è´¦å·: @{account.get('username')} (ä½¿ç”¨ {source})")
                    continue
                
                username = account.get('username')
                tweets = self.scan_account(account)
                
                if tweets:
                    results[username] = tweets
                
        finally:
            self._close_browser()
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    scraper = XPlaywrightScraper()
    
    print("\n" + "="*60)
    print("ğŸ”” X/Twitter Playwright æŠ“å–å™¨")
    print("="*60)
    
    # æ‰§è¡Œæ‰«æ
    results = scraper.scan_all()
    
    print(f"\nğŸ“Š Playwright æ‰«æç»“æœ:")
    total_tweets = sum(len(tweets) for tweets in results.values())
    print(f"   æ€»è´¦å·æ•°: {len([a for a in scraper.accounts if a.get('source') == 'playwright'])}")
    print(f"   ç­–ç•¥æ¨æ–‡: {total_tweets}")
    
    return results

if __name__ == "__main__":
    main()
